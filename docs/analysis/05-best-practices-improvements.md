# æœ€ä½³å¯¦è¸å’Œæ”¹é€²å»ºè­°

**æ–‡æª”ç‰ˆæœ¬**: v1.0
**æœ€å¾Œæ›´æ–°**: 2025-10-31
**ç›®æ¨™è®€è€…**: é–‹ç™¼åœ˜éšŠã€æ¶æ§‹å¸«ã€æŠ€è¡“Lead
**ç‹€æ…‹**: âœ… å·²å®Œæˆ

---

## ç›®éŒ„

1. [ç³»çµ±è¨­è¨ˆæœ€ä½³å¯¦è¸](#ç³»çµ±è¨­è¨ˆæœ€ä½³å¯¦è¸)
2. [çŸ­æœŸæ”¹é€²å»ºè­°ï¼ˆ1-2é€±ï¼‰](#çŸ­æœŸæ”¹é€²å»ºè­°)
3. [ä¸­æœŸæ”¹é€²å»ºè­°ï¼ˆ1-2å€‹æœˆï¼‰](#ä¸­æœŸæ”¹é€²å»ºè­°)
4. [é•·æœŸæ”¹é€²å»ºè­°ï¼ˆ3-6å€‹æœˆï¼‰](#é•·æœŸæ”¹é€²å»ºè­°)
5. [æ€§èƒ½å„ªåŒ–å»ºè­°](#æ€§èƒ½å„ªåŒ–å»ºè­°)
6. [å®‰å…¨åŠ å›ºå»ºè­°](#å®‰å…¨åŠ å›ºå»ºè­°)
7. [é–‹ç™¼è¦ç¯„](#é–‹ç™¼è¦ç¯„)

---

## ç³»çµ±è¨­è¨ˆæœ€ä½³å¯¦è¸

### 1.1 ç¾æœ‰æ¶æ§‹äº®é» â­

#### 1. é›™å±¤é€šä¿¡æ¶æ§‹
**äº®é»**: 4Gé›²ç«¯èª¿åº¦ + 2.4Gæœ¬åœ°å”ä½œ
**å„ªå‹¢**:
- âœ… é›²ç«¯çµ±ä¸€èª¿åº¦ï¼Œå…¨å±€æœ€å„ª
- âœ… æœ¬åœ°å”ä½œä½å»¶é²ï¼Œå¯¦æ™‚æ§åˆ¶
- âœ… åˆ†å±¤è§£è€¦ï¼Œå„å¸å…¶è·

**æœ€ä½³å¯¦è¸**:
```python
# æ˜ç¢ºå€åˆ†é›²ç«¯å’Œæœ¬åœ°è·è²¬

# é›²ç«¯è·è²¬ï¼ˆ4G gRPCï¼‰
class CloudController:
    def assign_task(self, robot_uid, task):
        """ä»»å‹™åˆ†é…å’Œèª¿åº¦"""
        pass

    def sync_status(self, robot_uid, status):
        """ç‹€æ…‹åŒæ­¥å’Œç›£æ§"""
        pass

# æœ¬åœ°è·è²¬ï¼ˆ2.4Gï¼‰
class LocalController:
    def authenticate_robot(self, robot_id):
        """Robotèº«ä»½é©—è­‰"""
        pass

    def control_door(self, action):
        """è‰™é–€å¯¦æ™‚æ§åˆ¶"""
        pass
```

#### 2. äº‹ä»¶é©…å‹•æ¶æ§‹
**äº®é»**: DAPR + TaskUpdateSignalçµ±ä¸€äº‹ä»¶

**å„ªå‹¢**:
- âœ… ç™¼å¸ƒè€…å’Œè¨‚é–±è€…è§£è€¦
- âœ… æ˜“æ–¼æ°´å¹³æ“´å±•
- âœ… æ”¯æŒç•°æ­¥è™•ç†

**æœ€ä½³å¯¦è¸**:
```python
# éµå¾ªäº‹ä»¶é©…å‹•æ¨¡å¼

# âœ… å¥½çš„åšæ³•ï¼šä½¿ç”¨DAPR Pubsub
await dapr_client.publish_event(
    pubsub_name="pubsub",
    topic_name="task_update_signal",
    data={"task_id": 123, "status": "COMPLETED"}
)

# âŒ é¿å…ï¼šç›´æ¥RPCèª¿ç”¨å¤šå€‹æœå‹™
# await notification_service.notify(task_id)
# await analytics_service.record(task_id)
# await monitoring_service.log(task_id)
```

#### 3. Actoræ¨¡å¼ç‹€æ…‹ç®¡ç†
**äº®é»**: æ¯å€‹Robot/Stationç¨ç«‹Actorå¯¦ä¾‹

**å„ªå‹¢**:
- âœ… ç‹€æ…‹è®Šæ›´é›†ä¸­ç®¡ç†
- âœ… é¿å…åˆ†æ•£å¼é–
- âœ… æ˜“æ–¼æ¨ç†å’Œèª¿è©¦

**æœ€ä½³å¯¦è¸**:
```python
# Actor IDå‘½åè¦ç¯„
actor_id = f"s{site_uid}.u{unit_uid}"  # ä¾‹: s504.u1010020021

# Actorå…§éƒ¨ç‹€æ…‹ç®¡ç†
class RobotActorV2:
    def __init__(self, actor_id):
        self.actor_id = actor_id
        self.state = {}  # Actorå…§éƒ¨ç‹€æ…‹

    async def ping(self):
        """æ¯ç§’æ›´æ–°ç‹€æ…‹åˆ°Redis"""
        await self.update_redis()

    async def update_redis(self):
        """çµ±ä¸€çš„ç‹€æ…‹æ›´æ–°å…¥å£"""
        await redis.hset(
            f"UnitConsensus@{self.actor_id}",
            mapping=self.state
        )
```

#### 4. å¤šå±¤æ•¸æ“šåŒæ­¥
**äº®é»**: DAPR EventBus + TimescaleDB + Redis + WebSocket

**å„ªå‹¢**:
- âœ… å¯¦æ™‚æ€§ï¼ˆDAPRã€Redisã€WebSocketï¼‰
- âœ… æŒä¹…åŒ–ï¼ˆTimescaleDBï¼‰
- âœ… å¿«é€ŸæŸ¥è©¢ï¼ˆRedisï¼‰
- âœ… æ­·å²åˆ†æï¼ˆTimescaleDBæ™‚åºå„ªåŒ–ï¼‰

**æœ€ä½³å¯¦è¸**:
```python
# å¯«å…¥ç­–ç•¥ï¼šå…ˆå¯«DBï¼Œå†æ›´æ–°Cache

async def complete_task(task_id):
    # 1. æŒä¹…åŒ–åˆ°æ•¸æ“šåº«
    await db.update_task(task_id, status='COMPLETED')

    # 2. æ›´æ–°Rediså¿«å–
    await redis.hset(f"task:{task_id}", "status", "COMPLETED")

    # 3. ç™¼å¸ƒäº‹ä»¶é€šçŸ¥
    await dapr.publish_event("task_update_signal", {...})

# è®€å–ç­–ç•¥ï¼šå„ªå…ˆå¾Cache

async def get_task_status(task_id):
    # 1. å…ˆæŸ¥Redis
    cached = await redis.hgetall(f"task:{task_id}")
    if cached:
        return cached

    # 2. Cache missï¼ŒæŸ¥æ•¸æ“šåº«
    task = await db.get_task(task_id)

    # 3. å¯«å…¥Cache
    await redis.hset(f"task:{task_id}", mapping=task)

    return task
```

---

## çŸ­æœŸæ”¹é€²å»ºè­°

### 2.1 æ•´åˆåˆ†æ•£å¼è¿½è¹¤ ğŸ”´é«˜å„ªå…ˆ

**å•é¡Œ**: ç•¶å‰ç¼ºå°‘åˆ†æ•£å¼è¿½è¹¤ï¼Œé›£ä»¥è¿½è¹¤è·¨æœå‹™èª¿ç”¨éˆè·¯

**å½±éŸ¿**:
- âš ï¸ æ€§èƒ½ç“¶é ¸é›£ä»¥å®šä½
- âš ï¸ æ•…éšœæ’æŸ¥è€—æ™‚é•·
- âš ï¸ ä¾è³´æ‰‹å‹•é—œè¯task_uuid

**è§£æ±ºæ–¹æ¡ˆ**: æ•´åˆDAPR + Zipkin

**å¯¦æ–½æ­¥é©Ÿ**:

#### Step 1: éƒ¨ç½²Zipkinï¼ˆé ä¼°ï¼š1å¤©ï¼‰

```yaml
# zipkin-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zipkin
  namespace: infrastructure
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zipkin
  template:
    metadata:
      labels:
        app: zipkin
    spec:
      containers:
      - name: zipkin
        image: openzipkin/zipkin:latest
        ports:
        - containerPort: 9411
        env:
        - name: STORAGE_TYPE
          value: "elasticsearch"  # å¯é¸ï¼ŒæŒä¹…åŒ–å­˜å„²
        - name: ES_HOSTS
          value: "http://elasticsearch:9200"
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2
            memory: 4Gi
---
apiVersion: v1
kind: Service
metadata:
  name: zipkin
  namespace: infrastructure
spec:
  selector:
    app: zipkin
  ports:
  - port: 9411
    targetPort: 9411
  type: ClusterIP
```

#### Step 2: é…ç½®DAPRè¿½è¹¤ï¼ˆé ä¼°ï¼š0.5å¤©ï¼‰

```yaml
# dapr-config-tracing.yaml
apiVersion: dapr.io/v1alpha1
kind: Configuration
metadata:
  name: tracing
  namespace: application
spec:
  tracing:
    samplingRate: "0.1"  # 10%æ¡æ¨£ï¼ˆç”Ÿç”¢ç’°å¢ƒï¼‰
    zipkin:
      endpointAddress: "http://zipkin.infrastructure.svc:9411/api/v2/spans"
```

#### Step 3: æ›´æ–°Deploymentä½¿ç”¨è¿½è¹¤é…ç½®ï¼ˆé ä¼°ï¼š0.5å¤©ï¼‰

```yaml
# åœ¨æ‰€æœ‰Deploymentçš„annotationsä¸­æ·»åŠ 
annotations:
  dapr.io/config: "tracing"
```

#### Step 4: æ‡‰ç”¨å±¤æ·»åŠ Trace Contextå‚³æ’­ï¼ˆé ä¼°ï¼š1å¤©ï¼‰

```python
# Pythonç¯„ä¾‹ï¼ˆä½¿ç”¨OpenTelemetryï¼‰
from opentelemetry import trace
from opentelemetry.exporter.zipkin.json import ZipkinExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# åˆå§‹åŒ–Tracer
zipkin_exporter = ZipkinExporter(
    endpoint="http://zipkin.infrastructure.svc:9411/api/v2/spans"
)
trace.set_tracer_provider(TracerProvider())
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(zipkin_exporter)
)

tracer = trace.get_tracer(__name__)

# åœ¨æ¥­å‹™ä»£ç¢¼ä¸­æ·»åŠ Span
async def assign_task(task_id, unit_uid):
    with tracer.start_as_current_span("assign_task") as span:
        span.set_attribute("task_id", task_id)
        span.set_attribute("unit_uid", unit_uid)

        # æ¥­å‹™é‚è¼¯
        await db.update_task(task_id, unit_uid=unit_uid)
        await dapr.publish_event(...)

        span.add_event("task_assigned")
```

**é æœŸæ”¶ç›Š**:
- âœ… ç«¯åˆ°ç«¯è¿½è¹¤ä»»å‹™æµç¨‹
- âœ… è­˜åˆ¥æ€§èƒ½ç“¶é ¸
- âœ… å¿«é€Ÿå®šä½æ•…éšœ
- âœ… å¯è¦–åŒ–æœå‹™ä¾è³´

**ç¸½å·¥ä½œé‡**: 2-3å¤©
**æŠ•è³‡å›å ±**: â­â­â­â­â­ æ¥µé«˜

### 2.2 å¢åŠ æ—¥èªŒä¿ç•™æœŸ ğŸŸ¡ä¸­å„ªå…ˆ

**å•é¡Œ**: ç•¶å‰æ—¥èªŒä¿ç•™æœŸåƒ…1å¤©ï¼Œç„¡æ³•å›æº¯æ­·å²å•é¡Œ

**è§£æ±ºæ–¹æ¡ˆ**: å¢åŠ åˆ°7å¤©

**å¯¦æ–½æ­¥é©Ÿ**:

```yaml
# ä¿®æ”¹æ‰€æœ‰é—œéµæœå‹™çš„æ—¥èªŒTTL
env:
  - name: aliyun_logs_app-delivery-rpc_ttl
    value: "7"  # å¾1å¤©æ”¹ç‚º7å¤©

  - name: aliyun_logs_jarvis-iot-v6_ttl
    value: "7"

  - name: aliyun_logs_app-delivery-actor-robot_ttl
    value: "7"
```

**æˆæœ¬å½±éŸ¿**: æ—¥èªŒå­˜å„²è²»ç”¨å¢åŠ ç´„7å€ï¼ˆä¼°ç®—ï¼š$50/æœˆ â†’ $350/æœˆï¼‰

**ç¸½å·¥ä½œé‡**: 0.5å¤©
**æŠ•è³‡å›å ±**: â­â­â­

### 2.3 éƒ¨ç½²Prometheus Server ğŸŸ¡ä¸­å„ªå…ˆ

**å•é¡Œ**: ç•¶å‰åƒ…æœ‰DAPR Sidecar metricsï¼Œç¼ºå°‘é›†ä¸­å¼Prometheus Server

**è§£æ±ºæ–¹æ¡ˆ**: éƒ¨ç½²Prometheus + AlertManager

**å¯¦æ–½æ­¥é©Ÿ**ï¼ˆä½¿ç”¨kube-prometheus-stackï¼‰:

```bash
# ä½¿ç”¨Helmå®‰è£
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace infrastructure \
  --set prometheus.prometheusSpec.retention=15d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi
```

**ç¸½å·¥ä½œé‡**: 1å¤©
**æŠ•è³‡å›å ±**: â­â­â­â­

---

## ä¸­æœŸæ”¹é€²å»ºè­°

### 3.1 é…ç½®æ­»ä¿¡ä½‡åˆ— ğŸŸ¡ä¸­å„ªå…ˆ

**å•é¡Œ**: å¤±æ•—ä»»å‹™è™•ç†ä¾è³´Cronjobï¼Œç„¡æ˜ç¢ºDLQ

**è§£æ±ºæ–¹æ¡ˆ**: DAPR Resiliency + Kafka DLQ

#### æ–¹æ¡ˆA: DAPR Resiliency

```yaml
# dapr-resiliency.yaml
apiVersion: dapr.io/v1alpha1
kind: Resiliency
metadata:
  name: app-resiliency
  namespace: application
spec:
  policies:
    retries:
      task-retry:
        policy: exponential
        maxRetries: 3
        initialInterval: 1s
        maxInterval: 30s
        multiplier: 2

    circuitBreakers:
      task-circuit-breaker:
        maxRequests: 5
        timeout: 30s
        trip: consecutiveFailures > 3

    timeouts:
      task-timeout:
        general: 60s

  targets:
    actors:
      RobotActorV2:
        retry: task-retry
        circuitBreaker: task-circuit-breaker
        timeout: task-timeout

    apps:
      app-delivery-rpc:
        retry: task-retry
        circuitBreaker: task-circuit-breaker
```

#### æ–¹æ¡ˆB: Kafka DLQ Topic

```python
# å‰µå»ºDLQ Topic
kafka-topics.sh --create \
  --bootstrap-server kafka:9092 \
  --topic prod-task-dlq \
  --partitions 3 \
  --replication-factor 2

# æ‡‰ç”¨ä»£ç¢¼è™•ç†
async def process_task_event(event):
    try:
        await handle_task_update(event)
    except Exception as e:
        # é‡è©¦3æ¬¡å¾Œç™¼é€åˆ°DLQ
        if retry_count >= 3:
            await kafka_producer.send(
                topic='prod-task-dlq',
                value=event,
                headers={
                    'original-topic': 'task_update_signal',
                    'error-message': str(e),
                    'retry-count': str(retry_count)
                }
            )
```

**ç¸½å·¥ä½œé‡**: 1-2é€±
**æŠ•è³‡å›å ±**: â­â­â­â­

### 3.2 å¯¦ä½œå‘Šè­¦ç³»çµ± ğŸŸ¡ä¸­å„ªå…ˆ

**è§£æ±ºæ–¹æ¡ˆ**: Prometheus AlertManager + Grafana

**é—œéµå‘Šè­¦è¦å‰‡**:

```yaml
# prometheus-alerts.yaml
groups:
- name: task-alerts
  rules:
  # ä»»å‹™å¤±æ•—ç‡å‘Šè­¦
  - alert: TaskFailureRateHigh
    expr: |
      rate(app_delivery_tasks_failed_total[5m])
      /
      rate(app_delivery_tasks_created_total[5m])
      > 0.1
    for: 5m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "ä»»å‹™å¤±æ•—ç‡éé«˜: {{ $value | humanizePercentage }}"
      description: "Site {{ $labels.site_uid }} æœ€è¿‘5åˆ†é˜ä»»å‹™å¤±æ•—ç‡è¶…é10%"
      runbook: "https://docs.company.com/runbooks/task-failure-rate-high"

  # Roboté›¢ç·šå‘Šè­¦
  - alert: RobotOfflineCount
    expr: |
      sum(jarvis_iot_units_online_total{type="robot"})
      < 80
    for: 10m
    labels:
      severity: critical
      team: operations
    annotations:
      summary: "Roboté›¢ç·šæ•¸é‡ç•°å¸¸"
      description: "åœ¨ç·šRobotæ•¸é‡: {{ $value }}ï¼Œå°‘æ–¼80å°"

  # Podé‡å•Ÿå‘Šè­¦
  - alert: PodRestartingTooOften
    expr: |
      rate(kube_pod_container_status_restarts_total[1h]) > 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.pod }} é‡å•Ÿéæ–¼é »ç¹"
      description: "æœ€è¿‘1å°æ™‚å¹³å‡æ¯10åˆ†é˜é‡å•Ÿ1æ¬¡"

  # æ•¸æ“šåº«é€£æ¥æ± å‘Šè­¦
  - alert: DatabaseConnectionPoolExhausted
    expr: |
      pg_stat_database_connections{datname="app_aiyo"}
      > 90
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "æ•¸æ“šåº«é€£æ¥æ± æ¥è¿‘è€—ç›¡"
      description: "ç•¶å‰é€£æ¥æ•¸: {{ $value }}/100"

  # Kafka Lagå‘Šè­¦
  - alert: KafkaConsumerLagHigh
    expr: |
      kafka_consumer_group_lag{group="prod-salt-events-cmdb-consumer-group"}
      > 10000
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Kafkaæ¶ˆè²»å»¶é²éé«˜"
      description: "Consumer group {{ $labels.group }} lag: {{ $value }}"
```

**å‘Šè­¦é€šçŸ¥é…ç½®**:

```yaml
# alertmanager-config.yaml
receivers:
- name: 'team-backend'
  email_configs:
  - to: 'backend-team@company.com'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/XXX/YYY/ZZZ'
    channel: '#backend-alerts'

- name: 'team-operations'
  email_configs:
  - to: 'ops-team@company.com'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/XXX/YYY/ZZZ'
    channel: '#ops-alerts'
  pagerduty_configs:
  - service_key: 'PAGERDUTY_SERVICE_KEY'

route:
  receiver: 'team-backend'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  routes:
  - match:
      severity: critical
    receiver: 'team-operations'
    repeat_interval: 5m
```

**ç¸½å·¥ä½œé‡**: 1é€±
**æŠ•è³‡å›å ±**: â­â­â­â­

### 3.3 å„ªåŒ–ç›£æ§å„€è¡¨æ¿ ğŸŸ¢ä½å„ªå…ˆ

**æ¨è–¦çš„Grafana Dashboard**:

#### Dashboard 1: ä»»å‹™åŸ·è¡Œç¸½è¦½

```json
{
  "title": "ä»»å‹™åŸ·è¡Œç¸½è¦½",
  "panels": [
    {
      "title": "ä»»å‹™å‰µå»º/å®Œæˆè¶¨å‹¢",
      "targets": [
        {
          "expr": "rate(app_delivery_tasks_created_total[5m])"
        },
        {
          "expr": "rate(app_delivery_tasks_completed_total[5m])"
        }
      ]
    },
    {
      "title": "ä»»å‹™æˆåŠŸç‡",
      "targets": [
        {
          "expr": "rate(app_delivery_tasks_completed_total[5m]) / rate(app_delivery_tasks_created_total[5m])"
        }
      ]
    },
    {
      "title": "å¹³å‡ä»»å‹™æ™‚é•· (ç§’)",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, app_delivery_task_duration_seconds_bucket)"
        }
      ]
    }
  ]
}
```

**ç¸½å·¥ä½œé‡**: 2-3å¤©
**æŠ•è³‡å›å ±**: â­â­â­

---

## é•·æœŸæ”¹é€²å»ºè­°

### 4.1 é·ç§»åˆ°OpenTelemetry ğŸŸ¡ä¸­å„ªå…ˆ

**ç›®æ¨™**: çµ±ä¸€Metrics/Logs/Tracesæ¡é›†

**å„ªå‹¢**:
- âœ… ä¾›æ‡‰å•†ä¸­ç«‹ï¼Œé¿å…é–å®š
- âœ… çµ±ä¸€SDKå’ŒAPI
- âœ… è‡ªå‹•åŒ–Instrumentation
- âœ… èˆ‡DAPRå¤©ç„¶é›†æˆ

**å¯¦æ–½è¨ˆåŠƒ**ï¼ˆ3-6å€‹æœˆï¼‰:

#### Phase 1: åŸºç¤è¨­æ–½æº–å‚™ï¼ˆ1å€‹æœˆï¼‰

```yaml
# éƒ¨ç½²OpenTelemetry Collector
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector
  namespace: infrastructure
spec:
  mode: daemonset
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
      prometheus:
        config:
          scrape_configs:
          - job_name: 'dapr-sidecars'
            kubernetes_sd_configs:
            - role: pod

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024

      attributes:
        actions:
        - key: cluster
          value: prod-k8s
          action: insert

    exporters:
      prometheus:
        endpoint: "prometheus:9090"
      jaeger:
        endpoint: "jaeger:14250"
        tls:
          insecure: true
      logging:
        loglevel: debug

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch, attributes]
          exporters: [jaeger, logging]
        metrics:
          receivers: [otlp, prometheus]
          processors: [batch]
          exporters: [prometheus]
```

#### Phase 2: æ‡‰ç”¨å±¤æ•´åˆï¼ˆ2å€‹æœˆï¼‰

```python
# Pythonæ‡‰ç”¨æ•´åˆOpenTelemetry
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.grpc import GrpcInstrumentorClient, GrpcInstrumentorServer
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor

# åˆå§‹åŒ–
trace.set_tracer_provider(TracerProvider())
otlp_exporter = OTLPSpanExporter(endpoint="otel-collector:4317")
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

# è‡ªå‹•Instrumentation
FastAPIInstrumentor().instrument()
GrpcInstrumentorClient().instrument()
GrpcInstrumentorServer().instrument()
RedisInstrumentor().instrument()
Psycopg2Instrumentor().instrument()

# æ‰‹å‹•Span
tracer = trace.get_tracer(__name__)

async def assign_task(task_id, unit_uid):
    with tracer.start_as_current_span("assign_task") as span:
        span.set_attribute("task_id", task_id)
        span.set_attribute("unit_uid", unit_uid)

        # è‡ªå‹•è¨˜éŒ„DBæŸ¥è©¢ã€Redisæ“ä½œã€gRPCèª¿ç”¨
        await db.update_task(...)
        await redis.hset(...)
        await actor_client.invoke_actor(...)
```

#### Phase 3: å„ªåŒ–å’Œæ¨å»£ï¼ˆ3å€‹æœˆï¼‰

- ç‚ºæ‰€æœ‰å¾®æœå‹™æ·»åŠ OpenTelemetry
- å„ªåŒ–æ¡æ¨£ç‡å’Œæ€§èƒ½
- å»ºç«‹ç›£æ§SOP

**ç¸½å·¥ä½œé‡**: 3-6å€‹æœˆ
**æŠ•è³‡å›å ±**: â­â­â­â­â­

### 4.2 å¯¦æ–½SLO/SLIç›£æ§ ğŸŸ¢ä½å„ªå…ˆ

**å®šç¾©æœå‹™ç´šåˆ¥æŒ‡æ¨™ï¼ˆSLIï¼‰**:

```yaml
# SLIå®šç¾©
SLIs:
  - name: task_success_rate
    description: "ä»»å‹™æˆåŠŸç‡"
    query: |
      sum(rate(app_delivery_tasks_completed_total[5m]))
      /
      sum(rate(app_delivery_tasks_created_total[5m]))

  - name: api_latency_p95
    description: "APIéŸ¿æ‡‰æ™‚é–“ P95"
    query: |
      histogram_quantile(0.95,
        rate(http_request_duration_seconds_bucket[5m])
      )

  - name: robot_availability
    description: "Robotå¯ç”¨ç‡"
    query: |
      sum(jarvis_iot_units_online_total{type="robot"})
      /
      sum(jarvis_iot_units_total{type="robot"})
```

**å®šç¾©æœå‹™ç´šåˆ¥ç›®æ¨™ï¼ˆSLOï¼‰**:

```yaml
# SLOå®šç¾©
SLOs:
  - name: task_success_rate_slo
    sli: task_success_rate
    target: 0.995  # 99.5%
    window: 30d

  - name: api_latency_slo
    sli: api_latency_p95
    target: 0.5  # 500ms
    window: 30d

  - name: robot_availability_slo
    sli: robot_availability
    target: 0.95  # 95%
    window: 7d
```

**ç¸½å·¥ä½œé‡**: 1-2å€‹æœˆ
**æŠ•è³‡å›å ±**: â­â­â­â­

---

## æ€§èƒ½å„ªåŒ–å»ºè­°

### 5.1 æ•¸æ“šåº«å„ªåŒ–

#### ç´¢å¼•å„ªåŒ–

```sql
-- ç‚ºå¸¸ç”¨æŸ¥è©¢æ·»åŠ ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_task_status_created
ON t_app_task_ledger2 (status, created_at DESC);

CREATE INDEX CONCURRENTLY idx_task_unit_status
ON t_app_task_ledger2 (unit_uid, status);

CREATE INDEX CONCURRENTLY idx_task_site_date
ON t_app_task_ledger2 (site_uid, created_at DESC);

-- TimescaleDBé€£çºŒèšåˆï¼ˆåŠ é€Ÿçµ±è¨ˆæŸ¥è©¢ï¼‰
CREATE MATERIALIZED VIEW task_hourly_stats
WITH (timescaledb.continuous) AS
SELECT
  time_bucket('1 hour', created_at) AS bucket,
  site_uid,
  status,
  COUNT(*) AS task_count,
  AVG(EXTRACT(EPOCH FROM (completed_at - assigned_at))) AS avg_duration_seconds
FROM t_app_task_ledger2
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY bucket, site_uid, status;

-- è‡ªå‹•åˆ·æ–°
SELECT add_continuous_aggregate_policy('task_hourly_stats',
  start_offset => INTERVAL '3 hours',
  end_offset => INTERVAL '1 hour',
  schedule_interval => INTERVAL '1 hour');
```

#### æŸ¥è©¢å„ªåŒ–

```python
# âŒ N+1æŸ¥è©¢å•é¡Œ
async def get_tasks_with_orders():
    tasks = await db.query("SELECT * FROM t_app_task_ledger2 WHERE status='EXECUTING'")
    for task in tasks:
        order = await db.query("SELECT * FROM t_app_order WHERE task_id=?", task.id)  # Næ¬¡æŸ¥è©¢
        task.order = order

# âœ… ä½¿ç”¨JOIN
async def get_tasks_with_orders():
    results = await db.query("""
        SELECT t.*, o.*
        FROM t_app_task_ledger2 t
        LEFT JOIN t_app_order o ON o.task_id = t.task_id
        WHERE t.status = 'EXECUTING'
    """)
    return results
```

### 5.2 Rediså„ªåŒ–

```python
# âŒ å¤šæ¬¡å¾€è¿”
async def get_robot_info(unit_uid):
    space = await redis.hget(f"UnitConsensus@s504.u{unit_uid}", "space")
    purpose = await redis.hget(f"UnitConsensus@s504.u{unit_uid}", "purpose")
    battery = await redis.hget(f"UnitConsensus@s504.u{unit_uid}", "battery")
    # 3æ¬¡ç¶²çµ¡å¾€è¿”

# âœ… ä½¿ç”¨Pipeline
async def get_robot_info(unit_uid):
    pipe = redis.pipeline()
    pipe.hgetall(f"UnitConsensus@s504.u{unit_uid}")
    result = await pipe.execute()
    return result[0]  # 1æ¬¡ç¶²çµ¡å¾€è¿”
```

### 5.3 gRPCå„ªåŒ–

```python
# âœ… å•Ÿç”¨gRPCå£“ç¸®
channel = grpc.aio.insecure_channel(
    'robot-rpc.aurotek.com:443',
    options=[
        ('grpc.default_compression_algorithm', grpc.Compression.Gzip),
        ('grpc.grpc.default_compression_level', grpc.CompressionLevel.high),
    ]
)

# âœ… å•Ÿç”¨KeepAlive
channel = grpc.aio.insecure_channel(
    'robot-rpc.aurotek.com:443',
    options=[
        ('grpc.keepalive_time_ms', 30000),
        ('grpc.keepalive_timeout_ms', 10000),
        ('grpc.keepalive_permit_without_calls', True),
    ]
)

# âœ… é€£æ¥æ± 
class GrpcClientPool:
    def __init__(self, size=10):
        self.channels = [
            grpc.aio.insecure_channel('robot-rpc.aurotek.com:443')
            for _ in range(size)
        ]
        self.index = 0

    def get_channel(self):
        channel = self.channels[self.index]
        self.index = (self.index + 1) % len(self.channels)
        return channel
```

---

## å®‰å…¨åŠ å›ºå»ºè­°

### 6.1 å¯†ç¢¼ç®¡ç† ğŸ”´é«˜å„ªå…ˆ

**å•é¡Œ**: Grafanaå¯†ç¢¼æ˜æ–‡å­˜åœ¨ConfigMap

```yaml
# âŒ ç•¶å‰åšæ³•
env:
  - name: GF_SECURITY_ADMIN_PASSWORD
    value: "SYbcvyNee7UKpq2poc"  # æ˜æ–‡å¯†ç¢¼
```

**è§£æ±ºæ–¹æ¡ˆ**: ä½¿ç”¨Kubernetes Secret

```yaml
# å‰µå»ºSecret
kubectl create secret generic grafana-admin \
  --from-literal=password=$(openssl rand -base64 32) \
  -n infrastructure

# ä½¿ç”¨Secret
env:
  - name: GF_SECURITY_ADMIN_PASSWORD
    valueFrom:
      secretKeyRef:
        name: grafana-admin
        key: password
```

**ç¸½å·¥ä½œé‡**: 1å¤©
**æŠ•è³‡å›å ±**: â­â­â­â­â­

### 6.2 ç¶²çµ¡ç­–ç•¥ ğŸŸ¡ä¸­å„ªå…ˆ

```yaml
# é™åˆ¶namespaceé–“é€šä¿¡
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: application-network-policy
  namespace: application
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system  # å…è¨±Traefik
    - namespaceSelector:
        matchLabels:
          name: spf  # å…è¨±IoTå±¤
    ports:
    - protocol: TCP
      port: 5000
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: infrastructure  # å…è¨±è¨ªå•åŸºç¤è¨­æ–½
```

### 6.3 RBACåŠ å›º ğŸŸ¡ä¸­å„ªå…ˆ

```yaml
# æœ€å°æ¬Šé™åŸå‰‡
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-delivery-rpc-sa
  namespace: application
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-delivery-rpc-role
  namespace: application
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list"]  # åƒ…è®€æ¬Šé™
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-delivery-rpc-rolebinding
  namespace: application
subjects:
- kind: ServiceAccount
  name: app-delivery-rpc-sa
roleRef:
  kind: Role
  name: app-delivery-rpc-role
  apiGroup: rbac.authorization.k8s.io
```

---

## é–‹ç™¼è¦ç¯„

### 7.1 ä»£ç¢¼è¦ç¯„

#### Pythonä»£ç¢¼é¢¨æ ¼

```python
# ä½¿ç”¨type hints
async def assign_task(
    task_id: int,
    unit_uid: int,
    task_type: str
) -> Dict[str, Any]:
    """åˆ†é…ä»»å‹™çµ¦Robotæˆ–Station

    Args:
        task_id: ä»»å‹™ID
        unit_uid: è¨­å‚™UID
        task_type: ä»»å‹™é¡å‹ï¼ˆDELIVERY, PATROLç­‰ï¼‰

    Returns:
        DictåŒ…å«successå’Œmessage

    Raises:
        RobotNotAvailableException: ç„¡å¯ç”¨Robot
    """
    pass

# ä½¿ç”¨asyncioæœ€ä½³å¯¦è¸
# âœ… å¥½çš„åšæ³•
tasks = [
    assign_task(1, 1001),
    assign_task(2, 1002),
    assign_task(3, 1003),
]
results = await asyncio.gather(*tasks)  # ä¸¦è¡ŒåŸ·è¡Œ

# âŒ é¿å…
for task_id, unit_uid in assignments:
    await assign_task(task_id, unit_uid)  # é †åºåŸ·è¡Œï¼Œæ…¢
```

#### gRPC Protoè¦ç¯„

```protobuf
// âœ… å¥½çš„åšæ³•
syntax = "proto3";

package app_delivery.v2;

option go_package = "github.com/company/app-delivery/v2;app_delivery_v2";

// ä½¿ç”¨æ˜ç¢ºçš„æ¶ˆæ¯åç¨±
message AssignTaskRequest {
  int64 task_id = 1;
  int64 unit_uid = 2;
  TaskType task_type = 3;  // ä½¿ç”¨æšèˆ‰
  google.protobuf.Timestamp deadline = 4;  // ä½¿ç”¨æ¨™æº–é¡å‹
  map<string, string> metadata = 5;
}

enum TaskType {
  TASK_TYPE_UNSPECIFIED = 0;
  TASK_TYPE_DELIVERY = 1;
  TASK_TYPE_PATROL = 2;
  TASK_TYPE_MAINTENANCE = 3;
}
```

### 7.2 Gitæäº¤è¦ç¯„

```bash
# ä½¿ç”¨Conventional Commits
feat(task): æ·»åŠ ä»»å‹™é‡è©¦æ©Ÿåˆ¶
fix(robot): ä¿®å¾©Robotç™»éŒ„å¤±æ•—å•é¡Œ
docs(api): æ›´æ–°gRPC APIæ–‡æª”
refactor(actor): é‡æ§‹Actorç‹€æ…‹ç®¡ç†
test(task): æ·»åŠ ä»»å‹™åˆ†é…å–®å…ƒæ¸¬è©¦
chore(deps): å‡ç´šDAPRåˆ°v1.10
```

### 7.3 ç›£æ§æŒ‡æ¨™è¦ç¯„

```python
# ä½¿ç”¨Prometheus clientåº«
from prometheus_client import Counter, Histogram, Gauge

# å®šç¾©metrics
task_created_total = Counter(
    'app_delivery_tasks_created_total',
    'Total number of tasks created',
    ['site_uid', 'task_type']
)

task_duration_seconds = Histogram(
    'app_delivery_task_duration_seconds',
    'Task execution duration in seconds',
    ['site_uid', 'task_type', 'status'],
    buckets=[10, 30, 60, 120, 300, 600, 1800, 3600]
)

robot_online_count = Gauge(
    'jarvis_iot_robot_online_count',
    'Number of online robots',
    ['site_uid']
)

# ä½¿ç”¨metrics
task_created_total.labels(site_uid='504', task_type='DELIVERY').inc()

with task_duration_seconds.labels(site_uid='504', task_type='DELIVERY', status='COMPLETED').time():
    await execute_task()
```

---

## ç¸½çµ

### å„ªå…ˆç´šçŸ©é™£

| æ”¹é€²é …ç›® | å„ªå…ˆç´š | å·¥ä½œé‡ | æŠ•è³‡å›å ± | å»ºè­°æ™‚é–“è¡¨ |
|---------|-------|--------|---------|-----------|
| æ•´åˆåˆ†æ•£å¼è¿½è¹¤ | ğŸ”´ é«˜ | 2-3å¤© | â­â­â­â­â­ | 1é€±å…§å®Œæˆ |
| ä¿®æ”¹Grafanaå¯†ç¢¼ | ğŸ”´ é«˜ | 0.5å¤© | â­â­â­â­â­ | ç«‹å³åŸ·è¡Œ |
| å¢åŠ æ—¥èªŒä¿ç•™æœŸ | ğŸŸ¡ ä¸­ | 0.5å¤© | â­â­â­ | 2é€±å…§å®Œæˆ |
| éƒ¨ç½²Prometheus Server | ğŸŸ¡ ä¸­ | 1å¤© | â­â­â­â­ | 2é€±å…§å®Œæˆ |
| é…ç½®æ­»ä¿¡ä½‡åˆ— | ğŸŸ¡ ä¸­ | 1-2é€± | â­â­â­â­ | 1å€‹æœˆå…§å®Œæˆ |
| å¯¦ä½œå‘Šè­¦ç³»çµ± | ğŸŸ¡ ä¸­ | 1é€± | â­â­â­â­ | 1å€‹æœˆå…§å®Œæˆ |
| é·ç§»OpenTelemetry | ğŸŸ¡ ä¸­ | 3-6å€‹æœˆ | â­â­â­â­â­ | Q2-Q3åŸ·è¡Œ |
| SLO/SLIç›£æ§ | ğŸŸ¢ ä½ | 1-2å€‹æœˆ | â­â­â­â­ | Q3åŸ·è¡Œ |

### å¿«é€Ÿå‹åˆ©ï¼ˆQuick Winsï¼‰

1. **ä¿®æ”¹Grafanaå¯†ç¢¼**ï¼ˆ0.5å¤©ï¼Œç«‹å³å¯åšï¼‰
2. **å¢åŠ æ—¥èªŒä¿ç•™æœŸ**ï¼ˆ0.5å¤©ï¼Œç«‹å³å¯åšï¼‰
3. **éƒ¨ç½²Zipkin**ï¼ˆ1å¤©ï¼Œé«˜åƒ¹å€¼ï¼‰
4. **æ·»åŠ é—œéµå‘Šè­¦è¦å‰‡**ï¼ˆ2å¤©ï¼Œé«˜åƒ¹å€¼ï¼‰

### é•·æœŸè·¯ç·šåœ–

**Q1**: åŸºç¤ç›£æ§å®Œå–„
- åˆ†æ•£å¼è¿½è¹¤
- Prometheus Server
- å‘Šè­¦ç³»çµ±

**Q2**: å¯é æ€§æå‡
- æ­»ä¿¡ä½‡åˆ—
- é‡è©¦æ©Ÿåˆ¶å„ªåŒ–
- æ€§èƒ½å„ªåŒ–

**Q3-Q4**: å¯è§€æ¸¬æ€§å‡ç´š
- OpenTelemetryé·ç§»
- SLO/SLIç›£æ§
- å®Œæ•´çš„å¯è§€æ¸¬æ€§å¹³å°

---

## ç›¸é—œæ–‡æª”

- [ç³»çµ±æ¶æ§‹ç¸½è¦½](./01-system-architecture-overview.md)
- [ä»»å‹™é–‰ç’°æµç¨‹æŠ€è¡“æ–‡æª”](./02-task-loop-technical-doc.md)
- [APIåƒè€ƒæ–‡æª”](./03-api-reference.md)
- [é‹ç¶­å’Œæ•…éšœæ’æŸ¥æ‰‹å†Š](./04-operations-troubleshooting.md)

---

**æ–‡æª”ç¶­è­·**: æœ¬æ–‡æª”æ‡‰æ ¹æ“šç³»çµ±æ¼”é€²å’Œå¯¦éš›å¯¦æ–½æƒ…æ³æŒçºŒæ›´æ–°ã€‚å»ºè­°æ¯å­£åº¦å¯©æŸ¥ä¸€æ¬¡å„ªå…ˆç´šå’Œé€²åº¦ã€‚
